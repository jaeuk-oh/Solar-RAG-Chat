import time
import os
import base64
import uuid
import tempfile
from typing import Dict, List, Any, Optional
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader

from langchain_upstage import ChatUpstage
from langchain_core.messages import HumanMessage, SystemMessage

from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from dotenv import load_dotenv
import streamlit as st

############# streamlit ë°°í¬ ì‹œ chromadbì™€ sqlite ë²„ì „ ì•ˆ ë§ìŒ ##################
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
##############################################################################

# .env íŒŒì¼ì—ì„œ upstage key ë°›ì•„ì˜¤ê¸°
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")


# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}
    st.session_state.uploaded_files = []
    st.session_state.vectorstore = None
    st.session_state.rag_chain = None


# ì„¸ì…˜ ID ì„¤ì •
session_id = st.session_state.id
client = None


# ì±„íŒ… ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜
def reset_chat() -> None:
    """ë‚˜ëˆ´ë˜ ëŒ€í™”ì™€ ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜
    """
    st.session_state.messages = []
    st.session_state.context = None


# ì½ì–´ì˜¨ PDF ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
def display_pdf(file_bytes, filename) -> None:
    """PDF íŒŒì¼ì„ ë°›ì•„ì™€ì„œ ë””ìŠ¤í”Œë ˆì´ í•´ì£¼ëŠ” í•¨ìˆ˜
    """
    st.markdown(f"### PDF Preview: {filename}")
    base64_pdf = base64.b64encode(file_bytes).decode("utf-8")
    pdf_display = f"""<iframe src="data:application/pdf;base64,{base64_pdf}" width="400" height="100%" type="application/pdf" style="height:100vh; width:100%"></iframe>"""
    st.markdown(pdf_display, unsafe_allow_html=True)


# ì—¬ëŸ¬ PDF íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
def process_multiple_pdfs(uploaded_files):
    """ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜
    """
    all_pages = []
    
    for uploaded_file in uploaded_files:
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                
                loader = PyPDFLoader(file_path)
                pages = loader.load_and_split()
                all_pages.extend(pages)
                
        except Exception as e:
            st.error(f"íŒŒì¼ {uploaded_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if all_pages:
        # ëª¨ë“  í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë§Œë“¤ê¸°
        vectorstore = Chroma.from_documents(all_pages, UpstageEmbeddings(model="solar-embedding-1-large"))
        retriever = vectorstore.as_retriever(k=2)
        
        chat = ChatUpstage(upstage_api_key=os.getenv("UPSTAGE_API_KEY"))
        
        contextualize_q_system_prompt = """ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."""
        
        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        
        # ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        history_aware_retriever = create_history_aware_retriever(
            chat, retriever, contextualize_q_prompt
        )
        
        qa_system_prompt = """ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”. ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.
        ## ë‹µë³€ ì˜ˆì‹œ
        ğŸ“ë‹µë³€ ë‚´ìš©:
        ğŸ“ì¦ê±°:
        {context}"""
        
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        
        return vectorstore, rag_chain
    
    return None, None


with st.sidebar:
    st.header(f"Add your documents!")
    
    # ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ ìˆ˜ì •
    uploaded_files = st.file_uploader(
        "Choose your `.pdf` files", 
        type="pdf", 
        accept_multiple_files=True
    )
    
    if uploaded_files:
        # íŒŒì¼ì´ ìƒˆë¡œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        current_file_names = [f.name for f in uploaded_files]
        previous_file_names = [f['name'] for f in st.session_state.uploaded_files]
        
        if current_file_names != previous_file_names:
            st.write("Indexing your documents ...")
            
            # ìƒˆë¡œìš´ íŒŒì¼ë“¤ ì €ì¥
            st.session_state.uploaded_files = [
                {'name': f.name, 'content': f.getvalue()} 
                for f in uploaded_files
            ]
            
            # ë²¡í„°ìŠ¤í† ì–´ì™€ RAG ì²´ì¸ ìƒì„±
            vectorstore, rag_chain = process_multiple_pdfs(uploaded_files)
            
            if vectorstore and rag_chain:
                st.session_state.vectorstore = vectorstore
                st.session_state.rag_chain = rag_chain
                st.success("Ready to Chat!")
            else:
                st.error("ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ
        st.write("**ì—…ë¡œë“œëœ íŒŒì¼ë“¤:**")
        for i, file_info in enumerate(st.session_state.uploaded_files):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"ğŸ“„ {file_info['name']}")
            with col2:
                if st.button("ë¯¸ë¦¬ë³´ê¸°", key=f"preview_{i}"):
                    st.session_state.preview_file = i
    
    # ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
    if hasattr(st.session_state, 'preview_file') and st.session_state.preview_file is not None:
        if st.session_state.preview_file < len(st.session_state.uploaded_files):
            file_info = st.session_state.uploaded_files[st.session_state.preview_file]
            display_pdf(file_info['content'], file_info['name'])


# ì›¹ì‚¬ì´íŠ¸ ì œëª© ì‘ì„±
st.title("Solar RAG Chatbot")

# ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
if st.session_state.uploaded_files:
    st.info(f"í˜„ì¬ {len(st.session_state.uploaded_files)}ê°œì˜ PDF íŒŒì¼ì´ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# ë©”ì„¸ì§€ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ë©”ì„¸ì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])

# ê¸°ë¡í•˜ëŠ” ëŒ€í™”ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì„¤ì •
MAX_MESSAGES_BEFORE_DELETION = 8

# ìœ ì €ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!"):
    # RAG ì²´ì¸ì´ ì¤€ë¹„ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if st.session_state.rag_chain is None:
        st.error("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!")
    else:
        # ì´ì „ ëŒ€í™”ì˜ ê¸¸ì´ í™•ì¸
        if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
            del st.session_state.messages[0]
            del st.session_state.messages[0]

        st.session_state.messages.append(
            {"role": "user","content": prompt}
        )
        with st.chat_message("user"):
            st.markdown(prompt)

        # AI ì˜ ë‹µë³€ì„ ë°›ì•„ì„œ session stateì— ì €ì¥í•˜ê³ , ë³´ì—¬ë„ ì¤˜ì•¼í•¨
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            result = st.session_state.rag_chain.invoke(
                {'input': prompt, 'chat_history': st.session_state.messages}
            )
            
            with st.expander("ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ"):
                st.write(result['context'])

            for chunk in result['answer'].split(" "):
                full_response += chunk + " "
                message_placeholder.markdown(full_response)
        
        st.session_state.messages.append(
            {"role": "assistant","content": full_response})